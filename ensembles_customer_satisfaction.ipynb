{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Ensembles for Customer Satisfaction Prediction"]},{"cell_type":"markdown","metadata":{},"source":["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n","\n","KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n","\n","* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n","\n","You will find instructions below about how to define each variable.\n","\n","Once you're happy with your code, upload your notebook to KATE to check your feedback."]},{"cell_type":"markdown","metadata":{},"source":["Businesses can improve their services by tailoring them to individual customers. One important factor is knowing when customers are dissatisfied. Based on their records, one can use machine learning tools to make predictions about which customers are more at risk of being dissatisfied than others. Such predictions allow for individualized actions that may help retain customers and will improve quality."]},{"cell_type":"markdown","metadata":{},"source":["In this assignment, we will build a prediction model for bank account owners' satisfaction. The record includes more than 300 features for each client, including variable related to their balance and which banking operations they have performed. Many of these variables are sparse; some numerical, some categorical. \n","\n","Ensemble methods based on decision trees, such as random forests and boosting algorithms, have been very successful in modeling such heterogeneous tabular data. To learn how these models work, you will implement them step-by-step, and see how the performance of your predictions improve."]},{"cell_type":"markdown","metadata":{},"source":["### Load the data"]},{"cell_type":"markdown","metadata":{},"source":["Load the data in `data/train_data.csv` with `pandas`. Inspect its content with `.head()`, `.shape` and other methods of your choice."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### Target variable\n","\n","The last column, named `TARGET`, is the variable to be predicted. `TARGET=1` represents a dissatisfied customer. Inspect the target column with `.value_counts()`. \n","\n","What is the proportion of dissatisfied customers? Is the dataset balanced or imbalanced?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Note on dataset properties\n","\n","As you can see, the dataset is highly imbalanced: there are only 2.6k positive entries and 63.4k negative entries. It definitely should be addressed in the models by introducing class_weight parameter where possible (there are different ways it can be done - feel free check it out in sklearn documentation).\n","\n","If that is not possible to introduce class weights for the model due to the model type, be ready to the permanent majority class vote in the output. This can be addressed by tweaking the model parameters."]},{"cell_type":"markdown","metadata":{},"source":["Separate the data into features `X` and target `y`. Split the data into training and validation sets, with validation set of 5000 samples, with stratified split to keep the same level of imbalance.\n","\n","*Hint: you may use `train_test_split()` for stratified splits.*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Basic modelling pipeline\n","\n","Implement a basic modelling pipeline for a Decision Tree Classifier, fitting the training data and printing the training and validation accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Note that the prediction score is quite high, even for this very simple model. Take a moment to think why this high score is not that significant.\n","\n","#### ROC curve metric\n","\n","Change your scoring metric to `roc_auc_score`, which calculates the area below the ROC curve of your **prediction probabilities**, instead of using the binary prediction decisions.\n","\n","*Hint: Use the probabilities for `y = True` (not `y = False`).*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### Baseline score for random predictions\n","\n","Calculate the ROC AUC for random uniform prediction probabilities. \n","\n","Is the Decision Tree better? Based on the training and validation scores, what is the problem with the Decision Tree model?\n","\n","*Hint: You can use `np.random.uniform`.*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Create a function named `test_model(model, X_train, y_train, X_test, y_test)` that performs the basic prediction pipeline, receiving as argument the model and data, fitting the training data, and returning the training and test prediction scores. Check that it works with the Decision Tree model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Optimizing decision trees "]},{"cell_type":"markdown","metadata":{},"source":["We can improve the prediction model by setting up the Decision Tree. Check the arguments available for the `DecisionTreeClassifier` class. \n","\n","Which arguments do you think could improve the validation score? Optimize your model by changing the meta-parameters. Inspect the most important meta-parameter by calculating the training and validation score for different values."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["To evaluate your models, we will test your data on a testing set. Load the test at `data/test_data.csv`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_data = pd.read_csv('data/test_data.csv')"]},{"cell_type":"markdown","metadata":{},"source":[" Calculate the prediction probabilities for the test data for the best Decision Tree, saving them in a variable named `dtc_preds`. `dtc_preds` should be an numpy array a single dimension."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Bagging and Random Forests"]},{"cell_type":"markdown","metadata":{},"source":["While Decision Trees are prone to overfitting, their ensemble can be powerful predictors. Random Forests are essentially Bagging ensembles of decision trees, using the average prediction of the multiple decision trees base models, each trained with a different set of data samples.\n","\n","You will create a Bagging model class, named `myBagging`, filling the class structure below.\n","\n","The `.fit()` method should fit each base model with a bootstrap sample of the data (with replacement), with data size proportional by the meta-parameter `subsample`. That is, if `subsample=0.5`, each base model should get half the total number of samples.\n","\n","The `.predict_proba()` method should estimate and average the prediction probabilities of the base models.\n","\n","*Hint: You can use the `resample()` function for creating bootstrap samples.*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class myBagging:\n","    def __init__(self, base_models, subsample = 1.):\n","        self.n_models = len(base_models)\n","        self.base_models = base_models\n","        self.subsample = subsample\n","        \n","    def fit(self, X, y):\n","        '''Loop over base models, generate a bootstrap sample of the data with 'resample()',\n","           and fit them to the data.\n","           \n","           To access the variables inside the myBagging class, use the 'self.' prefix, \n","           i.e. self.base_models, self.n_models and self.subsample\n","        '''\n","        pass\n","    \n","    def predict_proba(self, X):\n","        '''Return the ensemble predictions, given by the average prediction probability over base models.\n","           It should be an array with the length of the dataset.'''\n","        pass\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["Run and score a Random Forest, with 10 base Decision Trees, with maximum depth 10 and subsample 0.5. Use your `myBagging` class and `test_model()`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Extra-Trees\n","\n","Extremely Randomized Trees are decision trees in which, at each node split during training , only a fraction of the features is considered for the optimal split (e.g. for optimal Gini gain). This functionality is implemented on `sklearn` under the parameter `max_features`. \n","\n","Run and score a Extra-Trees version of your Random Forest, by changing the `max_features` parameter."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Sklearn comparison"]},{"cell_type":"markdown","metadata":{},"source":["For comparison, run and score the `sklearn` implementation, `RandomForestClassifier`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Optimize your Random Forest"]},{"cell_type":"markdown","metadata":{},"source":["Optimize your Random Forest meta-parameters, both of the myBagging and Decision Trees, and make your predictions for the test data, saving the predictions under `rf_preds`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Note that including more decision trees improve performance but increases the computational cost of training linearly. The `max_depth` and `max_features` arguments can heavily cut the training time, by reducing the tree size and number of features considered at each split."]},{"cell_type":"markdown","metadata":{},"source":["## Gradient Boosting"]},{"cell_type":"markdown","metadata":{},"source":["We will now implement a more sophisticated ensemble, Gradient Boosting, in which the base models are trained sequentially. Each new base model predicts what previous base models missed. \n","\n","As gradient boosting requires a continuous gradient, it can only use regression models for the base learner. \n","\n","For this exercise, we will perform regression directly on the 0-1 class labels, and treat the raw outputs as probabilities. \n","\n","We will try to setup the base models to optimise the MSE loss function against the class-labels, for which the gradient becomes simply the residual errors. \n","\n","When applied to probabilities, the MSE is known as the Brier score. \n","\n","Whilst performing this exercise, have a think about whether this is a robust approach. \n","\n","If not, what would you change either to your base-learners, meta-algorithm, or evaluation metrics to make this more robust?\n","\n","You will have a chance to implement your suggestions tomorrow!\n","\n","In the below structure, fill the `.fit()` and `.predict_proba()` functions. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class myGradientBoosting:\n","    \n","    def __init__(self, base_models, learning_rate=0.5):\n","        self.n_models = len(base_models)\n","        self.models = base_models\n","        self.learning_rate = learning_rate\n","    \n","    def fit(self, x, y):\n","        ''' The `.fit()` function should loop over each base model \n","         fitting it to the residual of the ensemble predictions so far, for the MSE loss:\n","         \n","         predictions = 0\n","         for each base model:\n","             residual = y - predictions   \n","             fit base model and make new predictions\n","             predictions = predictions + learning_rate * new_prediction \n","        '''\n","        pass\n","       \n","    def predict_proba(self, x):\n","        ''' Generate the ensemble prediction, by looping over each base model.\n","            Get their predictions and sum them, scaled by the learning rate.\n","        \n","            Trick: Regressor models return only one prediction (instead of two probabilities in the Classifiers).\n","                   To make your class compatible with test_model(), you can repeat the predictions, e.g.:\n","                   predictions.reshape(-1,1).repeat(2,axis=1)'''\n","        pass\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["Run and score a Gradient Boosting model, with 20 base decision trees, with maximum depth 5, maximum feature 0.5 and learning rate 0.5. Use your `myGradientBoosting` class and `test_model()`. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["For comparison, run and score the `sklearn` implementation, `GradientBoostingClassifier`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Optimize your myGradientBoosting and decision tree meta-parameters, and make your predictions for the test data, saving the predictions under `gb_preds`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Try to think about the difference between your implementation and the GradientBoostingClassifier.\n","\n","Are there any fundamental differences? If so, why?\n","\n","You could try looking at the distribution of your output probabilities for each model."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}